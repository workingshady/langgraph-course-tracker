{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "921ceb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def ensure_api_keys() -> tuple[str, str]:\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    required_vars = [\"GOOGLE_API_KEY\", \"TAVILY_API_KEY\"]\n",
    "    missing = []\n",
    "\n",
    "    for var in required_vars:\n",
    "        if not os.environ.get(var):\n",
    "            missing.append(var)\n",
    "\n",
    "    if missing:\n",
    "        missing_list = \", \".join(missing)\n",
    "        raise EnvironmentError(\n",
    "            f\"Missing required environment variables: {missing_list}. \"\n",
    "            \"Set them in your OS environment or .env file.\"\n",
    "        )\n",
    "\n",
    "    return os.environ[\"GOOGLE_API_KEY\"], os.environ[\"TAVILY_API_KEY\"]\n",
    "\n",
    "\n",
    "GOOGLE_API_KEY, TAVILY_API_KEY = ensure_api_keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2465e97",
   "metadata": {},
   "source": [
    "# **1. What `init_chat_model` Actually Does**\n",
    "\n",
    "`init_chat_model` is a **unified factory** that:\n",
    "\n",
    "1. Detects **which LLM provider** you want (OpenAI, Anthropic, Google, AWS, etc.)\n",
    "2. Loads the correct **provider-specific LangChain wrapper**\n",
    "3. Instantiates the model with the parameters you pass (temperature, max_tokens, etc.)\n",
    "4. Optionally builds a **runtime-configurable model** (model chosen dynamically per request)\n",
    "5. Returns a **BaseChatModel** object with standard methods:\n",
    "\n",
    "   * `.invoke()` â€” single request\n",
    "   * `.stream()` â€” streaming tokens\n",
    "   * `.batch()` â€” multiple requests at once\n",
    "\n",
    "This means you can write **one unified LangChain agent**, and switch models with zero code changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4392f43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parrots talk for a fascinating combination of reasons, driven by their **intelligence, social nature, and evolutionary adaptations.** It's not just about mimicking sounds; it's a complex behavior with multiple underlying motivations.\n",
      "\n",
      "Here's a breakdown of why parrots talk:\n",
      "\n",
      "**1. Social Bonding and Communication:**\n",
      "\n",
      "*   **Flock Behavior:** Parrots are highly social animals that live in flocks in the wild. Communication is crucial for maintaining group cohesion, warning of danger, finding food, and coordinating activities. Talking, or vocalizing, is their primary way of doing this.\n",
      "*   **Mimicry as a Social Tool:** In the wild, parrots often mimic the calls of other birds in their flock. This helps them blend in, identify members, and understand the social dynamics of their group. When they mimic human sounds, it's essentially an extension of this innate social behavior. They are trying to integrate into their \"flock\" (their human family).\n",
      "\n",
      "**2. Intelligence and Cognitive Abilities:**\n",
      "\n",
      "*   **Learning and Memory:** Parrots are incredibly intelligent birds with excellent memories. They can learn and retain a vast repertoire of sounds, including human words and phrases.\n",
      "*   **Problem-Solving:** Their intelligence allows them to understand that certain sounds can elicit specific responses from their human companions. They learn that saying \"hello\" might get them attention, or asking for a treat might result in a reward.\n",
      "*   **Cognitive Stimulation:** Talking can be a form of mental stimulation for parrots. It keeps their minds engaged and prevents boredom.\n",
      "\n",
      "**3. Attention Seeking and Reinforcement:**\n",
      "\n",
      "*   **Positive Reinforcement:** When a parrot says something and receives a positive reaction from its owner (praise, treats, interaction), it learns that this behavior is rewarding. This positive reinforcement encourages them to repeat the behavior.\n",
      "*   **Getting Needs Met:** Parrots can learn to associate specific words or phrases with their desires. They might learn to say \"water\" when they are thirsty or \"cracker\" when they want a snack.\n",
      "\n",
      "**4. Playfulness and Entertainment:**\n",
      "\n",
      "*   **Enjoyment of Sound:** Many parrots simply seem to enjoy making sounds and experimenting with different vocalizations. They might \"talk\" just for the fun of it, especially when they are feeling playful.\n",
      "*   **Mimicking for Fun:** They can also mimic sounds they find interesting or amusing, like the doorbell, a phone ringing, or even laughter.\n",
      "\n",
      "**5. Understanding and Context (to a degree):**\n",
      "\n",
      "*   **Not Just Random Mimicry:** While some mimicry is purely imitative, many parrots develop a surprising understanding of the context in which certain words or phrases are used. They might say \"goodbye\" when someone leaves or \"hello\" when someone enters.\n",
      "*   **Associative Learning:** They learn to associate words with objects, actions, or emotions. This is a form of associative learning, similar to how humans learn language.\n",
      "\n",
      "**In summary, parrots talk because:**\n",
      "\n",
      "*   **It's in their nature:** They are social creatures programmed to communicate and bond through vocalizations.\n",
      "*   **They are smart:** Their intelligence allows them to learn, remember, and understand the function of sounds.\n",
      "*   **They are rewarded:** They learn that talking gets them attention, treats, and interaction.\n",
      "*   **They enjoy it:** It's a form of play and mental stimulation.\n",
      "\n",
      "It's important to remember that while parrots can learn to speak, they don't possess the same grammatical understanding or abstract thought as humans. Their \"talking\" is a remarkable feat of mimicry and associative learning, driven by their innate social and cognitive abilities.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "    \"google_genai:gemini-2.5-flash-lite\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "response = model.invoke(\"Why do parrots talk?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97e6e19",
   "metadata": {},
   "source": [
    "\n",
    "# **2. Full Parameter Breakdown**\n",
    "# **`model`**\n",
    "\n",
    "### **Purpose**\n",
    "\n",
    "The model ID you want to use, optionally including the provider name.\n",
    "\n",
    "### **Accepted formats**\n",
    "\n",
    "1. `\"gpt-4o\"`\n",
    "2. `\"openai:gpt-4o\"`\n",
    "3. `\"google_genai:gemini-2.5-flash-lite\"`\n",
    "4. `\"anthropic:claude-3.5-sonnet\"`\n",
    "\n",
    "# **`configurable_fields`**\n",
    "Controls which parameters can be changed **at runtime**.\n",
    "### Modes\n",
    "| Value      | Meaning                                          |\n",
    "| ---------- | ------------------------------------------------ |\n",
    "| `None`     | Model is fixed after creation                    |\n",
    "| `\"any\"`    | ALL fields configurable (dangerous for security) |\n",
    "| List/Tuple | Only selected parameters configurable            |\n",
    "\n",
    "### **When this is useful**\n",
    "* Building a **UI allowing users to pick models**\n",
    "* Making an **API gateway** that routes dynamically\n",
    "* Experimenting with model sweeps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff8eb477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.chat_models.base._ConfigurableModel at 0x25b17d36570>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_chat_model(\n",
    "    \"gpt-4o\",\n",
    "    configurable_fields=(\"temperature\", \"model\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0ef8183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--586fb4bd-7132-4388-8391-2e2ba1a93c0a-0', usage_metadata={'input_tokens': 2, 'output_tokens': 10, 'total_tokens': 12, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then at runtime:\n",
    "model.invoke(\n",
    "    \"hello\",\n",
    "    config={ \"configurable\": { \"temperature\": 0.9 } }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce2235",
   "metadata": {},
   "source": [
    "\n",
    "# **`config_prefix`**\n",
    "\n",
    "Adds a prefix to runtime configuration keys.\n",
    "\n",
    "### **Why this matters**\n",
    "\n",
    "If multiple configurable models exist in the same agent pipeline, prefixes prevent collision.\n",
    "\n",
    "---\n",
    "\n",
    "# **`**kwargs` â€” Provider-specific parameters**\n",
    "\n",
    "This is where you pass typical generation parameters.\n",
    "\n",
    "### Standard kwargs (supported mostly everywhere):\n",
    "\n",
    "| Param         | Meaning                  |\n",
    "| ------------- | ------------------------ |\n",
    "| `temperature` | randomness               |\n",
    "| `max_tokens`  | max output tokens        |\n",
    "| `timeout`     | max request time         |\n",
    "| `max_retries` | retry attempts           |\n",
    "| `api_key`     | override environment key |\n",
    "| `base_url`    | custom server endpoint   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e82ba4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "configurable_model = init_chat_model(\n",
    "    \"gpt-4o\",\n",
    "    configurable_fields=\"any\",\n",
    "    config_prefix=\"foo\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10edc5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--b7447e91-1494-481d-9bd5-3f3e8a6b6f36-0', usage_metadata={'input_tokens': 2, 'output_tokens': 10, 'total_tokens': 12, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\n",
    "    \"hello\",\n",
    "    config={ \"configurable\": { \"foo_temperature\": 0.8 } }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c50cd580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(profile={'max_input_tokens': 1048576, 'max_output_tokens': 65536, 'image_inputs': True, 'audio_inputs': True, 'pdf_inputs': True, 'video_inputs': True, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'image_tool_message': True, 'tool_choice': True}, model='models/gemini-2.5-flash-lite', google_api_key=SecretStr('**********'), temperature=0.4, max_output_tokens=2048, max_retries=2, timeout=20.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000025B197E7440>, default_metadata=(), model_kwargs={})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "init_chat_model(\n",
    "    \"google_genai:gemini-2.5-flash-lite\",\n",
    "    temperature=0.4,\n",
    "    max_tokens=2048,\n",
    "    timeout=20,\n",
    "    max_retries=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26658e8",
   "metadata": {},
   "source": [
    "\n",
    "# **Return Type**\n",
    "\n",
    "`init_chat_model` returns one of two things:\n",
    "\n",
    "### **A. Normal fixed model**\n",
    "\n",
    "â†’ When `model` is supplied and `configurable_fields=None`.\n",
    "\n",
    "### **B. Configurable model wrapper**\n",
    "\n",
    "â†’ When `model` missing OR `configurable_fields` is used.\n",
    "\n",
    "This wrapper defers calculation of:\n",
    "\n",
    "* Model provider\n",
    "* Model name\n",
    "* All kwargs\n",
    "\n",
    "Until `invoke()` is called.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e21c1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--95e9cceb-71e2-440d-93b4-58207de056ed-0', usage_metadata={'input_tokens': 2, 'output_tokens': 10, 'total_tokens': 12, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## **Mode 1 â€” Fixed Model (most common)**\n",
    "\n",
    "model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\",\n",
    "temperature=0)\n",
    "model.invoke(\"hello\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f764dfe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--b8e7cfd6-2061-40d4-beae-2bcf959f202b-0', usage_metadata={'input_tokens': 2, 'output_tokens': 10, 'total_tokens': 12, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## **Mode 2 â€” Partially Configurable Model**\n",
    "\n",
    "model = init_chat_model(temperature=0)\n",
    "model.invoke(\n",
    "    \"hi\",\n",
    "    config={\"configurable\": {\"model\": \"google_genai:gemini-2.5-flash-lite\"}}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58b83b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--1924013d-c7f3-43ba-9d4e-18524a86142d-0', usage_metadata={'input_tokens': 2, 'output_tokens': 10, 'total_tokens': 12, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## **Mode 3 â€” Fully Configurable Model**\n",
    "\n",
    "model = init_chat_model(\n",
    "    \"google_genai:gemini-2.5-flash-lite\",\n",
    "    configurable_fields=\"any\",\n",
    "    config_prefix=\"foo\"\n",
    ")\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"foo_model\": \"google_genai:gemini-2.5-flash-lite\",\n",
    "        \"foo_temperature\": 0.8\n",
    "    }\n",
    "}\n",
    "model.invoke(\"hello\", config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65866115",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GetWeather' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## **Mode 4 â€” Tools + Configurable Model**\u001b[39;00m\n\u001b[32m      3\u001b[39m model = init_chat_model(\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mgpt-4o\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     configurable_fields=(\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel_provider\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m tool_model = model.bind_tools([\u001b[43mGetWeather\u001b[49m, GetPopulation])\n\u001b[32m     10\u001b[39m tool_model.invoke(\u001b[33m\"\u001b[39m\u001b[33mLA or NY?\u001b[39m\u001b[33m\"\u001b[39m, config={\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mclaude-3.5\u001b[39m\u001b[33m\"\u001b[39m}})\n",
      "\u001b[31mNameError\u001b[39m: name 'GetWeather' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "## **Mode 4 â€” Tools + Configurable Model**\n",
    "\n",
    "model = init_chat_model(\n",
    "    \"google_genai:gemini-2.5-flash-lite\",\n",
    "    configurable_fields=(\"model\", \"model_provider\")\n",
    ")\n",
    "\n",
    "tool_model = model.bind_tools([GetWeather, GetPopulation])\n",
    "\n",
    "tool_model.invoke(\"LA or NY?\", config={\"configurable\": {\"model\": \"claude-3.5\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c63585b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch  # updated at 1.0\n",
    "\n",
    "tavily_search = TavilySearch(max_results=5)\n",
    "\n",
    "data = tavily_search.invoke({\"query\": \"What is LangGraph?\"})\n",
    "search_docs = data.get(\"results\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4d41e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',\n",
       "  'title': 'LangGraph Tutorial: What Is LangGraph and How to Use It?',\n",
       "  'content': 'LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner. By managing the flow of data and the sequence of operations, LangGraph allows developers to focus on the high-level logic of their applications rather than the intricacies of agent coordination. Whether you need a chatbot that can handle various types of user requests or a multi-agent system that performs complex tasks, LangGraph provides the tools to build exactly what you need. LangGraph significantly simplifies the development of complex LLM applications by providing a structured framework for managing state and coordinating agent interactions.',\n",
       "  'score': 0.9581988,\n",
       "  'raw_content': None},\n",
       " {'url': 'https://www.geeksforgeeks.org/machine-learning/what-is-langgraph/',\n",
       "  'title': 'What is LangGraph? - GeeksforGeeks',\n",
       "  'content': 'LangGraph is an open-source framework built by LangChain that streamlines the creation and management of AI agent workflows. By treating workflows as interconnected nodes and edges, LangGraph offers a scalable, transparent and developer-friendly way to design advanced AI systems ranging from simple chatbots to multi-agent system. * ****Enhanced decision-making:**** Models relationships between nodes, enabling AI agents to learn from past actions and feedback. * ****langgraph:**** Framework for building graph-based AI workflows. * Build the workflow graph using LangGraph, adding nodes for classification and response, connecting them with edges and compiling the app. * Send each input through the workflow graph and returns the botâ€™s response, either a greeting or an AI-powered answer.',\n",
       "  'score': 0.95067275,\n",
       "  'raw_content': None},\n",
       " {'url': 'https://www.ibm.com/think/topics/langgraph',\n",
       "  'title': 'What is LangGraph? - IBM',\n",
       "  'content': '*   [Overview](https://www.ibm.com/think/topics/ai-agents#7281535) *   [Overview](https://www.ibm.com/think/topics/components-of-ai-agents#498277090) *   [Learning](https://www.ibm.com/think/topics/ai-agent-learning#498277087) *   [Tutorial: LangGraph ReAct agent](https://www.ibm.com/think/tutorials/deploy-langgraph-react-agent-manage-it-support-tickets-watsonx-ai#1287801557) *   [Overview](https://www.ibm.com/think/topics/ai-agent-protocols#1509394340) *   [Tutorial: LangGraph ReAct agent](https://www.ibm.com/think/tutorials/deploy-langgraph-react-agent-manage-it-support-tickets-watsonx-ai#80364620) *   [Overview](https://www.ibm.com/think/insights/ai-agent-governance#1268897081) *   [Overview](https://www.ibm.com/think/topics/ai-agent-use-cases#257779831) *   [Human resources](https://www.ibm.com/think/topics/ai-agents-in-human-resources#257779835) LangGraph, created by [LangChain](https://www.ibm.com/think/topics/langchain), is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an [AI agent workflow](https://www.ibm.com/think/topics/ai-agents). LangGraph is also built on several key technologies, including [LangChain,](https://www.ibm.com/think/topics/langchain) a Python framework for building AI applications. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including [chatbots](https://www.ibm.com/think/topics/chatbots), state graphs and [other agent-based systems](https://www.ibm.com/think/topics/multiagent-system).',\n",
       "  'score': 0.9466806,\n",
       "  'raw_content': None},\n",
       " {'url': 'https://www.shakudo.io/integrations/langgraph',\n",
       "  'title': 'What is LangGraph? Docs, Demo and How to Deploy - Shakudo',\n",
       "  'content': 'LangGraph is a framework for building stateful, multi-agent applications using large language models (LLMs). It extends the LangChain',\n",
       "  'score': 0.94401723,\n",
       "  'raw_content': None},\n",
       " {'url': 'https://www.analyticsvidhya.com/blog/2024/07/langgraph-revolutionizing-ai-agent/',\n",
       "  'title': 'What is LangGraph? - Analytics Vidhya',\n",
       "  'content': '* LangGraph is a library built on top of Langchain that is designed to facilitate the creation of cyclic graphs for large language model (LLM) â€“ based AI agents. The agent executor class in the Langchain framework was the main tool for building and executing AI agents before LangGraph. Generative AI| DeepSeek| OpenAI Agent SDK| LLM Applications using Prompt Engineering| DeepSeek from Scratch| Stability.AI| SSM & MAMBA| RAG Systems using LlamaIndex| Building LLMs for Code| Python| Microsoft Excel| Machine Learning| Deep Learning| Mastering Multimodal RAG| Introduction to Transformer Model| Bagging & Boosting| Loan Prediction| Time Series Forecasting| Tableau| Business Analytics| Vibe Coding in Windsurf| Model Deployment using FastAPI| Building Data Analyst AI Agent| Getting started with OpenAI o3-mini| Introduction to Transformers and Attention Mechanisms',\n",
       "  'score': 0.9393874,\n",
       "  'raw_content': None}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aac419fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello to you too! How can I help you today? ðŸ˜Š', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--e7ca0edc-8f08-4ff7-87fc-c81a84ce6c82-0', usage_metadata={'input_tokens': 3, 'output_tokens': 13, 'total_tokens': 16, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a message\n",
    "msg = HumanMessage(content=\"Hello world\", name=\"Lance\")\n",
    "\n",
    "# Message list\n",
    "messages = [msg]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f510ce79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
